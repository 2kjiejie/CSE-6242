{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(class_y):\n",
    "    # Input:            \n",
    "    #   class_y         : list of class labels (0's and 1's)\n",
    "    \n",
    "    # TODO: Compute the entropy for a list of classes\n",
    "    #\n",
    "    # Example:\n",
    "    #    entropy([0,0,0,1,1,1,1,1,1]) = 0.92\n",
    "        \n",
    "    entropy = 0\n",
    "    ### Implement your code here\n",
    "    #############################################\n",
    "    '''\n",
    "    c0 = 0\n",
    "    c1 = 0\n",
    "    for i in class_y:\n",
    "    \tif i == 0:\n",
    "    \t\tc0 += 1\n",
    "    \telif i == 1:\n",
    "    \t\tc1 += 1\n",
    "    \telse:\n",
    "    \t\tprint(\"only allowed 0 and 1!\")\n",
    "    '''\n",
    "    c0 = class_y.count(0)\n",
    "    c1 = class_y.count(1)\n",
    "    if len(class_y) == 0:\n",
    "        entropy = 0\n",
    "    else:\n",
    "        p0 = c0 / len(class_y)\n",
    "        p1 = c1 / len(class_y)\n",
    "        entropy = stats.entropy([p0,p1], base=2)\n",
    "    #############################################\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9182958340544894"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([0,0,0,1,1,1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(previous_y, current_y):\n",
    "    # Inputs:\n",
    "    #   previous_y: the distribution of original labels (0's and 1's)\n",
    "    #   current_y:  the distribution of labels after splitting based on a particular\n",
    "    #               split attribute and split value\n",
    "    \n",
    "    # TODO: Compute and return the information gain from partitioning the previous_y labels\n",
    "    # into the current_y labels.\n",
    "    # You will need to use the entropy function above to compute information gain\n",
    "    # Reference: http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15381-s06/www/DTs.pdf\n",
    "    \n",
    "    \"\"\"\n",
    "    Example:\n",
    "    \n",
    "    previous_y = [0,0,0,1,1,1]\n",
    "    current_y = [[0,0], [1,1,1,0]]\n",
    "    \n",
    "    info_gain = 0.45915\n",
    "    \"\"\"\n",
    "\n",
    "    info_gain = 0\n",
    "    ### Implement your code here\n",
    "    #############################################\n",
    "    info_gain = entropy(previous_y) - \\\n",
    "    ((len(current_y[0])/(len(current_y[0])+len(current_y[1])))*entropy(current_y[0]) + \\\n",
    "     (len(current_y[1])/(len(current_y[0])+len(current_y[1])))*entropy(current_y[1]))\n",
    "    #############################################\n",
    "    return info_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4591479170272448"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_gain([0,0,0,1,1,1],[[0,0], [1,1,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(1,int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_classes(X, y, split_attribute, split_val):\n",
    "    # Inputs:\n",
    "    #   X               : data containing all attributes\n",
    "    #   y               : labels\n",
    "    #   split_attribute : column index of the attribute to split on\n",
    "    #   split_val       : either a numerical or categorical value to divide the split_attribute\n",
    "    \n",
    "    # TODO: Partition the data(X) and labels(y) based on the split value - BINARY SPLIT.\n",
    "    # \n",
    "    # You will have to first check if the split attribute is numerical or categorical    \n",
    "    # If the split attribute is numeric, split_val should be a numerical value\n",
    "    # For example, your split_val could be the mean of the values of split_attribute\n",
    "    # If the split attribute is categorical, split_val should be one of the categories.   \n",
    "    #\n",
    "    # You can perform the partition in the following way\n",
    "    # Numeric Split Attribute:\n",
    "    #   Split the data X into two lists(X_left and X_right) where the first list has all\n",
    "    #   the rows where the split attribute is less than or equal to the split value, and the \n",
    "    #   second list has all the rows where the split attribute is greater than the split \n",
    "    #   value. Also create two lists(y_left and y_right) with the corresponding y labels.\n",
    "    #\n",
    "    # Categorical Split Attribute:\n",
    "    #   Split the data X into two lists(X_left and X_right) where the first list has all \n",
    "    #   the rows where the split attribute is equal to the split value, and the second list\n",
    "    #   has all the rows where the split attribute is not equal to the split value.\n",
    "    #   Also create two lists(y_left and y_right) with the corresponding y labels.\n",
    "\n",
    "    '''\n",
    "    Example:\n",
    "    \n",
    "    X = [[3, 'aa', 10],                 y = [1,\n",
    "         [1, 'bb', 22],                      1,\n",
    "         [2, 'cc', 28],                      0,\n",
    "         [5, 'bb', 32],                      0,\n",
    "         [4, 'cc', 32]]                      1]\n",
    "    \n",
    "    Here, columns 0 and 2 represent numeric attributes, while column 1 is a categorical attribute.\n",
    "    \n",
    "    Consider the case where we call the function with split_attribute = 0 and split_val = 3 (mean of column 0)\n",
    "    Then we divide X into two lists - X_left, where column 0 is <= 3  and X_right, where column 0 is > 3.\n",
    "    \n",
    "    X_left = [[3, 'aa', 10],                 y_left = [1,\n",
    "              [1, 'bb', 22],                           1,\n",
    "              [2, 'cc', 28]]                           0]\n",
    "              \n",
    "    X_right = [[5, 'bb', 32],                y_right = [0,\n",
    "               [4, 'cc', 32]]                           1]\n",
    "\n",
    "    Consider another case where we call the function with split_attribute = 1 and split_val = 'bb'\n",
    "    Then we divide X into two lists, one where column 1 is 'bb', and the other where it is not 'bb'.\n",
    "        \n",
    "    X_left = [[1, 'bb', 22],                 y_left = [1,\n",
    "              [5, 'bb', 32]]                           0]\n",
    "              \n",
    "    X_right = [[3, 'aa', 10],                y_right = [1,\n",
    "               [2, 'cc', 28],                           0,\n",
    "               [4, 'cc', 32]]                           1]\n",
    "               \n",
    "    ''' \n",
    "    \n",
    "    X_left = []\n",
    "    X_right = []\n",
    "    \n",
    "    y_left = []\n",
    "    y_right = []\n",
    "    ### Implement your code here\n",
    "    #############################################\n",
    "    if isinstance(split_val,(int,float)):\n",
    "        for i in range(len(X)):\n",
    "            if X[i][split_attribute] <= split_val:\n",
    "                X_left.append(X[i])\n",
    "                y_left.append(y[i])\n",
    "            else:\n",
    "                X_right.append(X[i])\n",
    "                y_right.append(y[i])\n",
    "    else:\n",
    "        for i in range(len(X)):\n",
    "            if X[i][split_attribute] == split_val:\n",
    "                X_left.append(X[i])\n",
    "                y_left.append(y[i])\n",
    "            else:\n",
    "                X_right.append(X[i])\n",
    "                y_right.append(y[i])\n",
    "\n",
    "\n",
    "    #############################################\n",
    "    return (X_left, X_right, y_left, y_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[3, 'aa', 10],[1, 'bb', 22],[2, 'cc', 28],[5, 'bb', 32],[4, 'cc', 32]]\n",
    "y = [1,1,0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[1, 'bb', 22], [5, 'bb', 32]],\n",
       " [[3, 'aa', 10], [2, 'cc', 28], [4, 'cc', 32]],\n",
       " [1, 0],\n",
       " [1, 0, 1])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition_classes(X,y,1,'bb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_split(X, y):\n",
    "    # Inputs:\n",
    "    #   X       : Data containing all attributes\n",
    "    #   y       : labels\n",
    "    # TODO: For each node find the best split criteria and return the \n",
    "    # split attribute, spliting value along with \n",
    "    # X_left, X_right, y_left, y_right (using partition_classes)\n",
    "    '''\n",
    "    \n",
    "    NOTE: Just like taught in class, don't use all the features for a node.\n",
    "    Repeat the steps:\n",
    "\n",
    "    1. Select m attributes out of d available attributes\n",
    "    2. Pick the best variable/split-point among the m attributes\n",
    "    3. return the split attributes, split point, left and right children nodes data \n",
    "\n",
    "    '''\n",
    "    split_attribute = 0\n",
    "    split_value = 0\n",
    "    X_left, X_right, y_left, y_right = [], [], [], []\n",
    "    ### Implement your code here\n",
    "    #############################################\n",
    "    best_en = 0\n",
    "    T = np.array(X)\n",
    "    X_left, X_right, y_left, y_right = [],[],[],[]\n",
    "    for i in range(len(X[0])):\n",
    "        if isinstance(X[0][i],(int,float)):\n",
    "            maxi = int(max(T[:,i]))\n",
    "            mini = int(min(T[:,i]))\n",
    "            for j in range(mini,(maxi + 1)):\n",
    "                X_left1, X_right1, y_left1, y_right1 = partition_classes(X,y,i,j)\n",
    "                info_i = information_gain(y,[y_left1,y_right1])\n",
    "                if info_i > best_en:\n",
    "                    best_en = info_i\n",
    "                    split_attribute = i\n",
    "                    split_val = j\n",
    "                    X_left = X_left1\n",
    "                    X_right = X_right1\n",
    "                    y_left = y_left1\n",
    "                    y_right = y_right1\n",
    "        else:\n",
    "            str_list = list(set(T[:,i]))\n",
    "            for j in str_list:\n",
    "                X_left1, X_right1, y_left1, y_right1 = partition_classes(X,y,i,j)\n",
    "                info_i = information_gain(y,[y_left1,y_right1])\n",
    "                if info_i > best_en:\n",
    "                    best_en = info_i\n",
    "                    split_attribute = i\n",
    "                    split_val = j\n",
    "                    X_left = X_left1\n",
    "                    X_right = X_right1\n",
    "                    y_left = y_left1\n",
    "                    y_right = y_right1\n",
    "\n",
    "    print(\"best_entropy: \",best_en)\n",
    "    \n",
    "    \n",
    "    #############################################\n",
    "    return split_attribute, split_val, X_left, X_right, y_left, y_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_entropy:  0.41997309402197514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " 22,\n",
       " [[3, 'aa', 10], [1, 'bb', 22]],\n",
       " [[2, 'cc', 28], [5, 'bb', 32], [4, 'cc', 32]],\n",
       " [1, 1],\n",
       " [0, 0, 1])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    def __init__(self, max_depth):\n",
    "        # Initializing the tree as an empty dictionary or list, as preferred\n",
    "        self.tree = {}\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def learn(self, X, y, par_node = {}, depth=0):\n",
    "        # TODO: Train the decision tree (self.tree) using the the sample X and labels y\n",
    "        # You will have to make use of the functions in utils.py to train the tree\n",
    "\n",
    "        # Use the function best_split in util.py to get the best split and \n",
    "        # data corresponding to left and right child nodes\n",
    "        \n",
    "        # One possible way of implementing the tree:\n",
    "        #    Each node in self.tree could be in the form of a dictionary:\n",
    "        #       https://docs.python.org/2/library/stdtypes.html#mapping-types-dict\n",
    "        #    For example, a non-leaf node with two children can have a 'left' key and  a \n",
    "        #    'right' key. You can add more keys which might help in classification\n",
    "        #    (eg. split attribute and split value)\n",
    "        ### Implement your code here\n",
    "        #############################################\n",
    "        if entropy(y) == 0 or self.max_depth == 0:\n",
    "            \n",
    "            self.tree['status']='leaf'\n",
    "            #self.tree['value']= y[0]\n",
    "\n",
    "            return\n",
    "\n",
    "        elif len(y)==0:\n",
    "            self.tree[\"status\"]=\"failed\"\n",
    "            \n",
    "        else:\n",
    "            split_attribute, split_val, X_left, X_right, y_left, y_right = best_split(X,y)\n",
    "            self.max_depth -= 1\n",
    "            left_tree = DecisionTree(self.max_depth)\n",
    "            right_tree = DecisionTree(self.max_depth)\n",
    "\n",
    "\n",
    "            left_tree.learn(X_left, y_left)\n",
    "            right_tree.learn(X_right, y_right)\n",
    "\n",
    "            self.tree['status'] = \"parent\"\n",
    "            self.tree['attr'] = split_attribute\n",
    "            self.tree['val'] = split_val\n",
    "            self.tree['left'] = left_tree.tree\n",
    "            self.tree['right'] = right_tree.tree\n",
    "        #############################################\n",
    "\n",
    "\n",
    "    def classify(self, record):\n",
    "        # TODO: classify the record using self.tree and return the predicted label\n",
    "        ### Implement your code here\n",
    "        #############################################\n",
    "        if self.tree['status']=='failed':\n",
    "            return self.tree['status']\n",
    "        elif self.tree['status']=='leaf':\n",
    "            return self.tree['status']\n",
    "        elif record[self.tree['attr']] <= self.tree['attr_val']:\n",
    "            return self.tree['left'].classify(record)\n",
    "\n",
    "        else:\n",
    "            return self.tree['right'].classify(record)\n",
    "        #############################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[3,  10],[1,  22],[2,  28],[5,  32],[4,  32]]\n",
    "y = [1,1,0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = DecisionTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.learn(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3, 10],\n",
       "       [ 1, 22],\n",
       "       [ 2, 28],\n",
       "       [ 5, 32],\n",
       "       [ 4, 32]])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41997309402197514\n",
      "0.2516291673878228\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "a = DecisionTree()\n",
    "a.learn(T,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'normal',\n",
       " 'attr': 1,\n",
       " 'attr_val': 23.200000000000003,\n",
       " 'left': {'status': 'leaf', 'value': 1},\n",
       " 'right': {'status': 'normal',\n",
       "  'attr': 1,\n",
       "  'attr_val': 28.0,\n",
       "  'left': {'status': 'leaf', 'value': 0},\n",
       "  'right': {'status': 'normal',\n",
       "   'attr': 0,\n",
       "   'attr_val': 4.0,\n",
       "   'left': {'status': 'leaf', 'value': 1},\n",
       "   'right': {'status': 'leaf', 'value': 0}}}}"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'parent',\n",
       " 'attr': 0,\n",
       " 'val': 2,\n",
       " 'left': {'status': 'leaf'},\n",
       " 'right': {'status': 'parent',\n",
       "  'attr': 0,\n",
       "  'val': 5,\n",
       "  'left': {'status': 'leaf'},\n",
       "  'right': {'status': 'leaf'}}}"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709505944546688"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "import ast\n",
    "import operator\n",
    "from scipy.stats import mode\n",
    "\n",
    "class DecisionTree(object):\n",
    "#(object):\n",
    "    def __init__(self):\n",
    "        # Initializing the tree as an empty dictionary or list, as preferred\n",
    "        #self.tree = []\n",
    "        self.tree = {}\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def learn(self, X, y):\n",
    "        # TODO: Train the decision tree (self.tree) using the the sample X and labels y\n",
    "        # You will have to make use of the functions in utils.py to train the tree\n",
    "        \n",
    "        # One possible way of implementing the tree:\n",
    "        #    Each node in self.tree could be in the form of a dictionary:\n",
    "        #       https://docs.python.org/2/library/stdtypes.html#mapping-types-dict\n",
    "        #    For example, a non-leaf node with two children can have a 'left' key and  a \n",
    "        #    'right' key. You can add more keys which might help in classification\n",
    "        #    (eg. split attribute and split value)\n",
    "\n",
    "        #figure out how many attributes are selected in this case\n",
    "\n",
    "        shape= np.shape(X)\n",
    "        attr_num=shape[1]\n",
    "        rows_num=shape[0]\n",
    "\n",
    "        if entropy(y)==0:\n",
    "            self.tree['status']='leaf'\n",
    "            self.tree['value']= y[0]\n",
    "\n",
    "            return\n",
    "\n",
    "        elif len(y)==0:\n",
    "            self.tree[\"status\"]=\"failed\"\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "        elif (X == X[0]).all():\n",
    "            self.tree['status']= 'leaf'\n",
    "            self.tree['value']= mode(y)\n",
    "            return\n",
    "\n",
    "        else:\n",
    "\n",
    "            #gain the max of attributes\n",
    "            #max_value= np.amax(X)\n",
    "            #min_value=np.amin(X)\n",
    "             #max_value=max(max_list)\n",
    "\n",
    "            info_rank={}\n",
    "\n",
    "\n",
    "            for col_index in range(attr_num):\n",
    "                info={}\n",
    "                X = np.array(X)\n",
    "                max_value= max(X[:,col_index])\n",
    "                min_value= min(X[:,col_index])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                for attr_val in np.linspace(min_value, max_value, 6):\n",
    "                \n",
    "\n",
    "                  inted=partition_classes(X,y,col_index, attr_val)\n",
    "                  x_left=inted[0]\n",
    "                  x_right=inted[1]\n",
    "                  y_left =inted[2]\n",
    "                  y_right =inted[3]\n",
    "                  if len(y_left) !=0  and len(y_right) !=0:\n",
    "\n",
    "                      y_current=[y_left,y_right]\n",
    "                      #print(y_current)\n",
    "\n",
    "                      info_gain = information_gain(y, y_current)\n",
    "                      info[attr_val]=info_gain\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "                if bool(info):\n",
    "                   best_key= max(info.items(), key=operator.itemgetter(1))[0]\n",
    "                   info_rank[col_index]=(best_key ,info[best_key])\n",
    "\n",
    "\n",
    "            \n",
    "            if bool(info)==False and bool(info_rank)==False:\n",
    "                print(X)\n",
    "                print(y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #detemine the optimal atrr to split up the data\n",
    "\n",
    "            optimal_attr=0\n",
    "            optimal_value=0\n",
    "            optimal_ig=0\n",
    "            max_en = 0\n",
    "            \n",
    "\n",
    "            for key, item in info_rank.items():\n",
    "                if item[1] > optimal_ig:\n",
    "                    optimal_value = item[0]\n",
    "                    optimal_attr= key\n",
    "                    max_en = item[1]\n",
    "\n",
    "                pass\n",
    "            \n",
    "            print(max_en)\n",
    "            \n",
    "            result = partition_classes(X,y, optimal_attr, optimal_value)\n",
    "            \n",
    "            #tree_carrier[\"split_info\"]=(optimal_attr, optimal_value)\n",
    "\n",
    "            left_tree= DecisionTree()\n",
    "            right_tree=DecisionTree()\n",
    "\n",
    "\n",
    "            left_tree.learn(result[0], result[2])\n",
    "            right_tree.learn(result[1], result[3])\n",
    "\n",
    "            self.tree['status']=\"normal\"\n",
    "            self.tree['attr']=optimal_attr\n",
    "            self.tree['attr_val']= optimal_value\n",
    "            self.tree['left']=left_tree.tree\n",
    "            self.tree['right']=right_tree.tree\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def classify(self, record):\n",
    "        # TODO: classify the record using self.tree and return the predicted label\n",
    "        if self.tree['status']=='failed':\n",
    "            return 0\n",
    "        elif self.tree['status']=='leaf':\n",
    "            return self.tree['value']\n",
    "        elif record[self.tree['attr']] <= self.tree['attr_val']:\n",
    "            return self.tree['left'].classify(record)\n",
    "\n",
    "        else:\n",
    "            return self.tree['right'].classify(record)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import ast\n",
    "\n",
    "class DecisionTree(object):\n",
    "    def __init__(self):\n",
    "        # Initializing the tree as an empty dictionary or list, as preferred\n",
    "        #self.tree = []\n",
    "        # 4 keys. Attributes, values, left(tree)-again a decision tree, right(tree)-again a decision tree\n",
    "        self.tree = {}\n",
    "\n",
    "    def learn(self, X, y):\n",
    "        # TODO: Train the decision tree (self.tree) using the the sample X and labels y\n",
    "        # You will have to make use of the functions in utils.py to train the tree\n",
    "        \n",
    "        # One possible way of implementing the tree:\n",
    "        #    Each node in self.tree could be in the form of a dictionary:\n",
    "        #       https://docs.python.org/2/library/stdtypes.html#mapping-types-dict\n",
    "        #    For example, a non-leaf node with two children can have a 'left' key and  a \n",
    "        #    'right' key. You can add more keys which might help in classification\n",
    "        #    (eg. split attribute and split value)\n",
    "\n",
    "\n",
    "        # convert list to numpy\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "\n",
    "        # if has only one class, or too many one class\n",
    "        y_counts = np.bincount(y.astype(int))\n",
    "        if len(y_counts == 1) or y_counts[0] == 0:\n",
    "            self.tree['class'] = np.argmax(y_counts)\n",
    "            return\n",
    "        if (y_counts[1]/y_counts[0] > 0.9) or (y_counts[1]/y_counts[0] < 0.1):\n",
    "            self.tree['class'] = np.argmax(y_counts)\n",
    "            return\n",
    "\n",
    "        # select the attribute where one classes max is far from the other's min\n",
    "        maxFeatDist = -1000000000\n",
    "        featID = -1\n",
    "        for i in range(len(X[0])):\n",
    "            # fetch feature\n",
    "            classOneFeat = [x[i] for index,x in enumerate(X) if y[index]==0]\n",
    "            classTwoFeat = [x[i] for index,x in enumerate(X) if y[index]==1]\n",
    "            # find max feature distance\n",
    "            dist1 = min(classOneFeat) - max(classTwoFeat)\n",
    "            dist2 = min(classTwoFeat) - min(classOneFeat)\n",
    "            if max([dist1, dist2]) > maxFeatDist:\n",
    "                maxFeatDist = max([dist1, dist2])\n",
    "                featID = i\n",
    "\n",
    "        # random select a featID\n",
    "        # featID = np.random.randint(0, len(X[0]))\n",
    "\n",
    "        # iterate through all possible value\n",
    "        X_left = []\n",
    "        X_right = []\n",
    "        y_left = []\n",
    "        y_right = []\n",
    "        splitValue = 0\n",
    "        infoGain = 0\n",
    "        X_downSampled = X[::50,:]\n",
    "        for i in range(len(X_downSampled)):\n",
    "            X_left_cur, X_right_cur, y_left_cur, y_right_cur = partition_classes(X, y, featID, X_downSampled[i][featID])\n",
    "            infoGain_cur = information_gain(y, [y_left_cur, y_right_cur])\n",
    "            if infoGain_cur > infoGain:\n",
    "                splitValue = X[i][featID]\n",
    "                infoGain = infoGain_cur\n",
    "                X_left = X_left_cur\n",
    "                X_right = X_right_cur\n",
    "                y_left = y_left_cur\n",
    "                y_right = y_right_cur\n",
    "\n",
    "        # if infoGain is zero: return majority\n",
    "        if infoGain == 0:\n",
    "            # y_1 = [value for value in y if value == 1]\n",
    "            # y_0 = [value for value in y if value == 0]\n",
    "            # if len(y_1) >= len(y_0):\n",
    "            #     class_Majority = 1\n",
    "            # else:\n",
    "            #     class_Majority = 0\n",
    "            self.tree['class'] = np.argmax(y_counts)\n",
    "            return\n",
    "\n",
    "        # save current splitAttribute, splitValue. Construct a new tree for both sides\n",
    "        self.tree[\"splitAttr\"] = featID\n",
    "        self.tree[\"splitValue\"] = splitValue\n",
    "        self.tree[\"left\"] = DecisionTree()\n",
    "        self.tree[\"right\"] = DecisionTree()\n",
    "        # learn for both sides\n",
    "        self.tree[\"left\"].learn(X_left, y_left)\n",
    "        self.tree[\"right\"].learn(X_right, y_right)\n",
    "\n",
    "\n",
    "    def classify(self, record):\n",
    "        # TODO: classify the record using self.tree and return the predicted label\n",
    "        class_predict = -1\n",
    "        if 'class' in self.tree:\n",
    "            class_predict = self.tree['class']\n",
    "        else:\n",
    "            splitAttr_cur = self.tree['splitAttr']\n",
    "            if record[splitAttr_cur] <= self.tree['splitValue']:\n",
    "                class_predict = self.tree['left'].classify(record)\n",
    "            else:\n",
    "                class_predict = self.tree['right'].classify(record)\n",
    "        return class_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
